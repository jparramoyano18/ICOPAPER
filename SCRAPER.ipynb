{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create string with all the dates in which there is a snapshot\n",
    "\n",
    "import datetime\n",
    "\n",
    "start = datetime.datetime.strptime(\"2013-04-28\", \"%Y-%m-%d\")\n",
    "end = datetime.datetime.strptime(\"2018-09-09\", \"%Y-%m-%d\")\n",
    "date_generated = [start + datetime.timedelta(days=x) for x in range(0, (end-start).days,7)]\n",
    "\n",
    "list_of_dates = []\n",
    "for date in date_generated:\n",
    "    list_of_dates.append(date.strftime(\"%Y-%m-%d\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stringlist = (','.join(list_of_dates)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = stringlist.replace(\"-\",\"\")\n",
    "dates = dates.split(\",\")\n",
    "\n",
    "base = 'https://coinmarketcap.com/historical/'\n",
    "\n",
    "snaps = []\n",
    "for x in dates:\n",
    "       snaps.append((base + x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load what we need\n",
    "\n",
    "import json\n",
    "import requests\n",
    "\n",
    "# Install selenium\n",
    "\n",
    "# In terminal: pip install selenium\n",
    "#http://selenium-python.readthedocs.io/locating-elements.html\n",
    "\n",
    "# Set up folder environment\n",
    "import os\n",
    "import sys\n",
    "import zipfile\n",
    "\n",
    "# Where are we?\n",
    "\n",
    "# Great, that is the path I want\n",
    "path = os.getcwd()\n",
    "path\n",
    "\n",
    "#/Users/jose/pp4rs/2018-uzh-course-material copia/14-project/Coinmarketcap scraper\n",
    "\n",
    "# Now I want to create a folder of where to store my output\n",
    "# Give it a name. Need to specify different path if you are on a Mac.\n",
    "output_path = path + '\\\\scraper_output'\n",
    "\n",
    "# Create a folder (if it doesnt exist yet!)\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)\n",
    "    \n",
    "# Great. We have an output folder\n",
    "\n",
    "# Set up shop for scraping\n",
    "\n",
    "# Import some useful stuff\n",
    "from selenium import webdriver\n",
    "\n",
    "# To transmit password and username\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "# This is helpful for setting path environment to store files\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "# To not overload the system\n",
    "import time\n",
    "\n",
    "# Define variables needed for scraping\n",
    "# Set chromedriver options so that it saves the zip file in the current directory\n",
    "# https://stackoverflow.com/questions/18026391/setting-chrome-preferences-w-selenium-webdriver-in-python\n",
    "chromeOptions = webdriver.ChromeOptions()\n",
    "\n",
    "# Specify where I want to store the downloads\n",
    "# pass this download directory to my Chrome options\n",
    "my_prefs = {\"download.default_directory\" : output_path}\n",
    "chromeOptions.add_experimental_option(\"prefs\", my_prefs)\n",
    "\n",
    "# Set driver options\n",
    "# Specify the path to where I stored Chromedriver\n",
    "chromedriver = '/Users/jose/Desktop/DESKTOP/chromedriver'\n",
    "\n",
    "# Start the chromedriver! \n",
    "# Specify where to find the driver and which preferences to choose\n",
    "driver = webdriver.Chrome(executable_path = chromedriver, \n",
    "                          chrome_options = chromeOptions)\n",
    "\n",
    "# Alternatively and easier is just to say without any special prefs:\n",
    "#chromeOptions.add_experimental_option(\"prefs\",prefs)\n",
    "\n",
    "import requests\n",
    "import lxml.html as lh\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 0\n",
    "for x in snaps:\n",
    "\n",
    "    #Create a handle, page, to handle the contents of the website\n",
    "    page = requests.get(x)\n",
    "\n",
    "    #Make the text more bautiful:\n",
    "    soup = bs(page.text, \"lxml\")\n",
    "    \n",
    "    #Store the contents of the website under doc\n",
    "    doc = lh.fromstring(page.content)\n",
    "\n",
    "    #Parse data that are stored between <tr>..</tr> of HTML\n",
    "    tr_elements = doc.xpath('//tr')\n",
    "\n",
    "    col=[]\n",
    "    i=0\n",
    "\n",
    "    #For each row, store each first element (header) and an empty list\n",
    "    for t in tr_elements[0]:\n",
    "        i+=1\n",
    "        name=t.text_content()\n",
    "        col.append((name,[]))\n",
    "    \n",
    "    #Since out first row is the header, data is stored on the second row onwards\n",
    "    for j in range(1,len(tr_elements)):\n",
    "        #T is our j'th row\n",
    "        T=tr_elements[j]\n",
    "    \n",
    "        #If row is not of size 11, the //tr data is not from our table \n",
    "        if len(T)!=11:\n",
    "            break\n",
    "    \n",
    "        #i is the index of our column\n",
    "        i=0\n",
    "    \n",
    "        #Iterate through each element of the row\n",
    "        for t in T.iterchildren():\n",
    "            data=t.text_content() \n",
    "            #Check if row is empty\n",
    "            if i>0:\n",
    "            #Convert any numerical value to integers\n",
    "                try:\n",
    "                    data=int(data)\n",
    "                except:\n",
    "                    pass\n",
    "            #Append the data to the empty list of the i'th column\n",
    "            col[i][1].append(data)\n",
    "            #Increment i for the next column\n",
    "            i+=1\n",
    "        \n",
    "    # Create dataframe\n",
    "\n",
    "    Dict={title:column for (title,column) in col}\n",
    "    df=pd.DataFrame(Dict)\n",
    "\n",
    "    df = df.replace('\\\\n', '', regex=True)\n",
    "\n",
    "    df = df.drop(df.columns[[0,10]], axis=1) \n",
    "\n",
    "    # Now store the data\n",
    "\n",
    "    import os\n",
    "    os.getcwd()\n",
    "    dir = os.getcwd()\n",
    "    df.to_csv(\"/Users/jose/Desktop/PAPERS/ICO PAPER/DATA/Original scraped data/\" + dates[k] + \".csv\")\n",
    "    k += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names=[]\n",
    "for t in dates:\n",
    "    column_names.append('Price_'+t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
